{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65eb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importação das bibliotecas necessárias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "##Método para captura das informações dos jogadores, do TransferMarket\n",
    "def get_player_info(club_url):\n",
    "\n",
    "#Definição dos headers para request\n",
    "    headers = {'User-Agent': 'Custom5'}\n",
    "    response = requests.get(club_url, headers = headers)\n",
    "\n",
    "#Utilização do Soup para parse do HTML obtido via request\n",
    "    soup = bs(response.text,'html.parser')\n",
    "    \n",
    "    url_baseline = \"https://www.transfermarkt.com\"\n",
    "    \n",
    "    lista_jogadores = []\n",
    "    lista_idades = []\n",
    "    lista_posicoes = []\n",
    "    lista_valores = []\n",
    "\n",
    "#Utilização do soup para buscar objetos que retornem listas e que contenham cada uma das informações desejadas\n",
    "    html_url = soup.find_all(\"div\",{\"class\":\"di nowrap\"})\n",
    "    jogadores = soup.find_all(\"img\",{\"class\":\"bilderrahmen-fixed lazy lazy\"})\n",
    "    idades = soup.find_all(\"td\",{\"class\":\"zentriert\"})\n",
    "    posicoes = soup.find_all(\"td\", {\"title\":[\"Goalkeeper\",\"Defender\",\"midfield\",\"attack\"]})\n",
    "    valores = soup.find_all(\"td\", {\"class\":\"rechts hauptlink\"})\n",
    "\n",
    "# obtenção do nome da equipe\n",
    "    nome_da_equipe = str(soup.find_all(\"div\", {\"class\":\"subkategorie-header\"})).split(\"</h2>\",1)[0].split(\"Squad of \",1)[1].rstrip()\n",
    "\n",
    "# obtenção das idaddes dos jogadores\n",
    "    j = 1\n",
    "    k = 1\n",
    "    while j<= len(jogadores):\n",
    "        try:\n",
    "            lista_idades.append(str(idades[k]).split(\"(\")[1].split(\")\")[0])\n",
    "        except:\n",
    "            lista_idades.append(\"N/A\")\n",
    "        j+=1\n",
    "        k+=3\n",
    "\n",
    "\n",
    "#obtenção do nome dos jogadores\n",
    "    \n",
    "    lista_semi_url = []\n",
    "    for element in html_url:\n",
    "        lista_semi_url.append(str(element).split('a href=\"')[1].split('\" title=')[0])\n",
    "    \n",
    "    lista_semi_url = list(dict.fromkeys(lista_semi_url))\n",
    "    \n",
    "    for semi_url in lista_semi_url:\n",
    "        full_player_url = url_baseline + semi_url\n",
    "        player_response = requests.get(full_player_url, headers = headers)\n",
    "        player_soup = bs(player_response.text,'html.parser')\n",
    "        player_html_url = player_soup.find(\"span\",{\"class\":\"info-table__content info-table__content--bold\"})\n",
    "        player_full_name = str(player_html_url).split('bold\">')[1].split('</span>')[0]\n",
    "        lista_jogadores.append(unicodedata.normalize('NFD',player_full_name).encode('ascii', 'ignore').decode(\"utf-8\"))\n",
    "\n",
    "# obtenção das posições dos jogadores\n",
    "    i=0\n",
    "    for i in range(0,len(jogadores)):\n",
    "        lista_posicoes.append(str(posicoes[i]).split('\"><div class=\"',1)[0].split('title=\"',1)[1])\n",
    "\n",
    "# obtenção dos valores dos jogadores\n",
    "    for i in range(0,len(valores)):\n",
    "        try:\n",
    "            lista_valores.append((str(valores[i]).split('\"><a')[1].split('\">')[1].split(\"</a>\")[0]))\n",
    "        except:\n",
    "            lista_valores.append(\"N/A\")\n",
    "\n",
    "# consolidando as info acima em um dataset\n",
    "    df_final = pd.DataFrame({\"Equipe\": nome_da_equipe,\n",
    "                             \"Jogador\": lista_jogadores,\n",
    "                             \"Posicao\": lista_posicoes,\n",
    "                             \"Idade\": lista_idades,\n",
    "                             \"Valor_de_Mercado\" : lista_valores})\n",
    "    return df_final, nome_da_equipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9de57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geração do CSV com os dados do scrapper\n",
    "def generate_csv(dataframe, local_path, nome_da_equipe, year):\n",
    "    try:\n",
    "        dataframe.to_csv(local_path + \"/\" + nome_da_equipe.replace(\" \", \"\") + \"_\" + str(year) + \".csv\" , index=False)\n",
    "        print(\"Excel gerado com sucesso no caminho informado\")\n",
    "    except Exception as e:\n",
    "        print(\"Houve um erro na geração do arquivo Excel --> \" + e)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f631f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação de folder que irá armazenar os dados obtidos via scrapper\n",
    "def create_folder(local_path, folder_name):\n",
    "    full_path_name = local_path + \"/\" + folder_name\n",
    "    flag_path_exists = os.path.exists(full_path_name)\n",
    "    if not flag_path_exists:\n",
    "        os.makedirs(full_path_name)\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616aeccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
